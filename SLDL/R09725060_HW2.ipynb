{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 統計學習與深度學習 (Fall, 2021) Homework 2\n",
    "R09725060  黃瀚陞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一題 [Data Preprocessing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('adultdata.csv', newline='') as csvfile:\n",
    "    rows = csv.reader(csvfile, delimiter=',',skipinitialspace=True)\n",
    "    train = np.asarray(list(rows),dtype=None)\n",
    "    \n",
    "with open('adulttest.csv', newline='') as csvfile:\n",
    "    rows = csv.reader(csvfile, delimiter=',',skipinitialspace=True)\n",
    "    test = np.asarray(list(rows),dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['39' 'State-gov' '77516' ... '40' 'United-States' '<=50K']\n",
      " ['50' 'Self-emp-not-inc' '83311' ... '13' 'United-States' '<=50K']\n",
      " ['38' 'Private' '215646' ... '40' 'United-States' '<=50K']\n",
      " ...\n",
      " ['58' 'Private' '151910' ... '40' 'United-States' '<=50K']\n",
      " ['22' 'Private' '201490' ... '20' 'United-States' '<=50K']\n",
      " ['52' 'Self-emp-inc' '287927' ... '40' 'United-States' '>50K']]\n",
      "[['25' 'Private' '226802' ... '40' 'United-States' '<=50K.']\n",
      " ['38' 'Private' '89814' ... '50' 'United-States' '<=50K.']\n",
      " ['28' 'Local-gov' '336951' ... '40' 'United-States' '>50K.']\n",
      " ...\n",
      " ['38' 'Private' '374983' ... '50' 'United-States' '<=50K.']\n",
      " ['44' 'Private' '83891' ... '40' 'United-States' '<=50K.']\n",
      " ['35' 'Self-emp-inc' '182148' ... '60' 'United-States' '>50K.']]\n"
     ]
    }
   ],
   "source": [
    "# delete the rows that contain \"?\" (missing value)\n",
    "train=train[~np.any(train==\"?\", axis=1)]\n",
    "print(train)\n",
    "test=test[~np.any(test == \"?\", axis=1)]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the order of columns same as adult50kp['columnname']\n",
    "train_sorted=np.copy(train)\n",
    "train_sorted[:,0]=train[:,11] #capital-loss\n",
    "train_sorted[:,1]=train[:,12] #hours-per-week\n",
    "train_sorted[:,2]=train[:,10] #capital-gain\n",
    "train_sorted[:,3]=train[:,4] #education-num\n",
    "train_sorted[:,4]=train[:,0] #age\n",
    "train_sorted[:,5]=train[:,2] #fnlwgt\n",
    "train_sorted[:,6]=train[:,7] #relationship\n",
    "train_sorted[:,7]=train[:,8] #race\n",
    "train_sorted[:,8]=train[:,9] #sex\n",
    "train_sorted[:,9]=train[:,6] #occupation\n",
    "train_sorted[:,10]=train[:,3] #education\n",
    "train_sorted[:,11]=train[:,13] #native-country\n",
    "train_sorted[:,12]=train[:,1] #workclass\n",
    "train_sorted[:,13]=train[:,5] #marital-status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sorted=np.copy(test)\n",
    "test_sorted[:,0]=test[:,11]\n",
    "test_sorted[:,1]=test[:,12]\n",
    "test_sorted[:,2]=test[:,10]\n",
    "test_sorted[:,3]=test[:,4]\n",
    "test_sorted[:,4]=test[:,0]\n",
    "test_sorted[:,5]=test[:,2]\n",
    "test_sorted[:,6]=test[:,7]\n",
    "test_sorted[:,7]=test[:,8]\n",
    "test_sorted[:,8]=test[:,9]\n",
    "test_sorted[:,9]=test[:,6]\n",
    "test_sorted[:,10]=test[:,3]\n",
    "test_sorted[:,11]=test[:,13]\n",
    "test_sorted[:,12]=test[:,1]\n",
    "test_sorted[:,13]=test[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "xscaler=preprocessing.StandardScaler().fit(train_sorted[:,:6])\n",
    "train_sorted[:,:6]=xscaler.transform(train_sorted[:,:6])\n",
    "test_sorted[:,:6]=xscaler.transform(test_sorted[:,:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding\n",
    "numeric_column=[\"capital-loss\",\"hours-per-week\",\"capital-gain\",\"education-num\",\"age\",\"fnlwgt\"]\n",
    "categorical_column=[\"relationship\",\"race\",\"sex\",\"occupation\",\"education\",\"native-country\",\"workclass\",\"marital-status\",\"y_label\"]\n",
    "train_sorted=pd.DataFrame(train_sorted,columns=numeric_column+categorical_column)\n",
    "test_sorted=pd.DataFrame(test_sorted,columns=numeric_column+categorical_column)\n",
    "total_category=pd.concat([train_sorted[categorical_column],test_sorted[categorical_column]],axis=0)\n",
    "total_dum=pd.get_dummies(total_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num=train_sorted[numeric_column]\n",
    "test_num=test_sorted[numeric_column]\n",
    "\n",
    "train_dum=total_dum[:30162]\n",
    "y_train=train_dum['y_label_>50K'].to_numpy().astype(float)\n",
    "train_dum=train_dum.drop(['y_label_<=50K','y_label_>50K','y_label_<=50K.','y_label_>50K.'],axis=1)\n",
    "test_dum=total_dum[30162:]\n",
    "y_test=test_dum['y_label_>50K.'].to_numpy().astype(float)\n",
    "test_dum=test_dum.drop(['y_label_<=50K','y_label_>50K','y_label_<=50K.','y_label_>50K.'],axis=1)\n",
    "\n",
    "x_train=pd.concat([train_num,train_dum],axis=1).to_numpy().astype(float)\n",
    "x_test=pd.concat([test_num,test_dum],axis=1).to_numpy().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rare columns\n",
    "rare_column=[]\n",
    "for i in range(6,len(x_train[0])):\n",
    "    if np.sum(x_train[:,i])<10:\n",
    "        rare_column.append(i)\n",
    "\n",
    "x_train=np.delete(x_train,rare_column,axis=1)\n",
    "x_test=np.delete(x_test,rare_column,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train match!\n",
      "x_test match!\n",
      "y_train match!\n",
      "y_test match!\n"
     ]
    }
   ],
   "source": [
    "adult50k={'x_train':x_train,'x_test':x_test,'y_train':y_train,'y_test':y_test}\n",
    "elems = ['x_train', 'x_test', 'y_train', 'y_test']\n",
    "\n",
    "for aelem in elems:\n",
    "    #cnomatch = np.sum(adult50kp[aelem] != adult50k[aelem])\n",
    "    cnomatch = np.sum(np.round(adult50kp[aelem],8) != np.round(adult50k[aelem],8))\n",
    "    if cnomatch == 0:\n",
    "        print(aelem, \"match!\")\n",
    "    else:\n",
    "        print(aelem, \"%d elements no match!\" % cnomatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.21858598, -0.07773411,  0.14609228,  1.12891838,  0.04279571,\n",
       "       -1.0627216 ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  1.        ,  0.        ,  1.        ,  1.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult50kp['x_train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.21858598, -0.07773411,  0.14609228,  1.12891838,  0.04279571,\n",
       "       -1.0627216 ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  1.        ,  0.        ,  1.        ,  1.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult50k['x_train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "補充說明：  \n",
    "在比較處理完的資料與題目所附的pickle檔案資料時有發現連續變數的欄位列印出來看起來一樣，但是直接比較卻會顯示為不一樣，推測應該可能是連續變數直接使用浮點數與浮點數的比較時會有不相同的狀況發生，所以我在比較時有先把值都統一取到小數點後第八位，比較結果就顯示相同了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二題 [ROC and AUC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.848340\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "    \n",
    "#train prediction model    \n",
    "c = 0.3\n",
    "lr2 = LogisticRegression(solver = 'lbfgs', C= c, max_iter = 1000)\n",
    "lr2.fit(adult50kp['x_train'], adult50kp['y_train'])\n",
    "#make prediction\n",
    "ypred = lr2.predict(adult50kp['x_test'])\n",
    "ypredprob = lr2.predict_proba(adult50kp['x_test'])\n",
    "#compute accuracy\n",
    "ncorrect = np.sum(adult50kp['y_test'] == ypred)\n",
    "accuracy_sk = ncorrect / adult50kp['y_test'].shape[0]\n",
    "print(\"Accuracy = %f\" % accuracy_sk)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2.1 ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_positive=ypredprob[:,0]\n",
    "true_class=adult50kp['y_test'].copy()\n",
    "\n",
    "sort_index=np.argsort(-p_positive)\n",
    "p_positive=p_positive[sort_index]\n",
    "true_class=true_class[sort_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPr_list=[]\n",
    "FPr_list=[]\n",
    "\n",
    "for t in range(100,-1,-1):\n",
    "    threshold=t/100\n",
    "    pred_class=np.zeros(len(p_positive))\n",
    "    \n",
    "    for i in range(len(p_positive)):\n",
    "        if p_positive[i]>=threshold:\n",
    "            pred_class[i]=0\n",
    "        else:\n",
    "            pred_class[i]=1\n",
    "\n",
    "    tp=fn=fp=tn=0\n",
    "    \n",
    "    for j in range(len(p_positive)):\n",
    "        if true_class[j]==0:\n",
    "            if pred_class[j]==0: \n",
    "                tp=tp+1\n",
    "            else: \n",
    "                fn=fn+1\n",
    "        else: \n",
    "            if pred_class[j]==0: \n",
    "                fp=fp+1\n",
    "            else:\n",
    "                tn=tn+1\n",
    "    \n",
    "    TPr=tp/(tp+fn)\n",
    "    FPr=fp/(fp+tn)\n",
    "    TPr_list.append(TPr)\n",
    "    FPr_list.append(FPr)\n",
    "    #print(\"threshold:\",threshold,\"rate:\",TPr,FPr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(FPr_list,TPr_list)\n",
    "plt.xlabel('FP Rate')\n",
    "plt.ylabel('TP Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2.2 ROC Curve的AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9033141891891892\n"
     ]
    }
   ],
   "source": [
    "area=0\n",
    "for i in range(len(TPr_list)-1):\n",
    "    area=area+1/2*(TPr_list[i+1]+TPr_list[i])*(FPr_list[i+1]-FPr_list[i])\n",
    "print(\"AUC:\",area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三題 [Logistic Regression with L2 Regularization]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3.1 Derive the gradient and hessian matrix for the new E(w)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient:  \n",
    "$ \\nabla E(w) = \\sum_{n=1}^N (y_n - t_n) \\phi_n + \\Lambda \\mathbf{w} = \\Phi^T (\\mathbf{y}-\\mathbf{t}) + \\Lambda \\mathbf{w}$  \n",
    "  \n",
    "Hessian:  \n",
    "$ H=\\nabla\\nabla E(w)=\\sum_{n=1}^N y_n (1-y_n) \\phi_n\\phi_n^T + \\Lambda \\mathbf{I}=\\Phi^T\\mathbf{R}\\Phi+\\Lambda\\mathbf{I}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3.2 Create your mylogistic_l2 class. Train your model and show the learned $w$ as well as test accuracy for the cases below. If $w$ is too long for you, show selected $w$ for continuous-valued, binary-valued, and the constant term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "    \n",
    "Y_train=adult50kp['y_train']\n",
    "X_train=adult50kp['x_train']\n",
    "Y_test=adult50kp['y_test']\n",
    "X_test=adult50kp['x_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mylogistic_l2():\n",
    "    def __init__(self, reg_vec, max_iter = 100, tol = 1e-5, add_intercept = True):\n",
    "        \"\"\"reg_vec: the regularization coefficient vector\n",
    "           max_iter: maximum number of iteration to run for the Newton method\n",
    "           tol: tolerance for the objective function\n",
    "           add_intercept: whether to add intercept (a column of ones) at last column of the feature matrix\"\"\"\n",
    "        ### Add your code here\n",
    "        \n",
    "        self.reg_vec=reg_vec\n",
    "        self.reg_matrix=None\n",
    "        self.max_iter=max_iter\n",
    "        self.tol=tol\n",
    "        self.add_intercept=add_intercept\n",
    "        self.X_train=None\n",
    "        self.Y_train=None\n",
    "        self.weight=None       \n",
    "\n",
    "    def fit(self, x, y, verbal = False):\n",
    "        #Add your code here   \n",
    "        \n",
    "        if(self.add_intercept):\n",
    "            x=np.c_[x,np.ones(len(x))] #add intercept column to x\n",
    "            #self.reg_vec=np.append(self.reg_vec,[0])\n",
    "        \n",
    "        n,m=x.shape\n",
    "        reg_matrix=np.zeros((m,m))\n",
    "\n",
    "        \n",
    "        for i in range(len(self.reg_vec)):\n",
    "            reg_matrix[i][i]=self.reg_vec[i]\n",
    "            \n",
    "        self.reg_matrix=reg_matrix\n",
    "        self.X_train=x\n",
    "        self.Y_train=y\n",
    "        n,m=self.X_train.shape  \n",
    "        \n",
    "        b=np.average(self.reg_vec)\n",
    "        I=np.identity(m)\n",
    "        \n",
    "        w_current=np.dot(np.dot(np.linalg.inv(np.dot(self.X_train.T,self.X_train)+b*I),self.X_train.T),self.Y_train)\n",
    "        w_small=w_current.copy()\n",
    "        \n",
    "        pred_temp=1/(1+np.exp(-1*np.dot(w_current,self.X_train.T)))\n",
    "        error_min=(-1)*(np.dot(self.Y_train.T,np.log(pred_temp))+np.dot(1-self.Y_train,np.log(1-pred_temp)))\\\n",
    "                    +1/2*np.dot(np.dot(w_current.T,self.reg_matrix),w_current)\n",
    "        error_last=error_min\n",
    "        \n",
    "        count=0\n",
    "        while(count<self.max_iter):\n",
    "            \n",
    "            pred_temp=1/(1+np.exp(-1*np.dot(w_current,self.X_train.T))) #prediction of this run\n",
    "            \n",
    "            R=np.identity(n)\n",
    "            for i in range(len(R)):\n",
    "                R[i][i]=pred_temp[i]*(1-pred_temp[i])\n",
    "\n",
    "            gradient=np.dot(self.X_train.T,pred_temp-self.Y_train)+np.dot(self.reg_matrix,w_current)\n",
    "            hessian=np.dot(np.dot(self.X_train.T,R),self.X_train)+self.reg_matrix\n",
    "            w_current=w_current-np.dot(np.linalg.inv(hessian),gradient)\n",
    "            \n",
    "            error_current=(-1)*(np.dot(self.Y_train.T,np.log(pred_temp))+np.dot(1-self.Y_train,np.log(1-pred_temp)))\\\n",
    "                            +(1/2)*np.dot(np.dot(w_current.T,self.reg_matrix),w_current)\n",
    "            \n",
    "            \n",
    "            error_difference=abs(error_last-error_current)\n",
    "            error_last=error_current\n",
    "            \n",
    "            if error_current<error_min:\n",
    "                error_min=error_current\n",
    "                w_small=w_current.copy()\n",
    "            if error_difference<self.tol:\n",
    "                break;\n",
    "            \n",
    "            if verbal:\n",
    "                print(\"SMALLEST ERROR:\",error_min,\"CURRENT ERROR:\",error_current)\n",
    "            count=count+1\n",
    "\n",
    "        self.weight=w_small\n",
    "#         print(self.weight)\n",
    "                \n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"doing prediction\"\"\"\n",
    "        ### add your code here.\n",
    "        w_current=self.weight\n",
    "        x=np.c_[x,np.ones(len(x))]\n",
    "        pred=1/(1+np.exp(-1*np.dot(w_current,x.T))) #prediction of this run\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i]>=0.5:\n",
    "                pred[i]=1\n",
    "            else:\n",
    "                pred[i]=0\n",
    "        return pred\n",
    "    \n",
    "    def CalAccuracy(self, y_pred, y_true):\n",
    "        correct=0\n",
    "        for i in range(len(y_pred)):\n",
    "            if(y_pred[i]==y_true[i]):\n",
    "                correct=correct+1       \n",
    "        return correct/len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.847875166002656\n",
      "WEIGHT:\n",
      " [ 0.25831075  0.35295138  2.33390153  0.75114521  0.33352443  0.07923687\n",
      " -0.259306   -0.03310589 -0.8020923  -1.16328381 -0.15748024  1.06974336\n",
      " -0.63384606  0.11673241 -0.23156738 -0.51712221 -0.07972165 -1.0994978\n",
      " -0.24602709  0.06196949  0.12668588  0.86265606 -0.91835284 -0.62122618\n",
      " -0.20074022 -0.75160098 -1.61011588  0.57582091  0.64899528  0.35374143\n",
      "  0.71721847 -0.02844947 -0.00095482 -0.1965409  -0.14635164  0.62694627\n",
      "  0.44820708  0.02459458  0.04692237 -0.49106775 -0.20303542 -0.16330368\n",
      " -0.01766235 -0.11132832 -0.09946182 -1.17391916  0.18070268 -0.069272\n",
      "  0.9764969   0.4609886  -0.49544041 -1.27203531  0.48677241 -0.89896373\n",
      " -0.06005426 -0.35084885  0.43281522  0.59412015  0.58215192 -0.62096228\n",
      " -0.05974804  0.09290352 -0.1518921  -0.00538529  0.03416091 -0.28908824\n",
      "  0.15605391  0.49540124  0.89094226  0.14915144  0.34248478 -0.31331216\n",
      " -0.35593911 -0.36249461 -0.66724748 -0.40883113  0.44748983  0.13776893\n",
      "  0.14135123 -0.11601542 -0.05610327 -0.93458304 -0.02925965 -0.29901296\n",
      " -0.15051125  0.35233187 -0.78584654  0.58020021  0.49704231 -0.19032074\n",
      " -0.00034772  0.1749938  -0.4882027  -0.31225962 -1.02643023 -0.72231084\n",
      "  1.4467247   1.15520747 -0.68020291 -1.21195631 -0.79833851 -0.53464849\n",
      " -1.34552489]\n"
     ]
    }
   ],
   "source": [
    "n,m=X_train.shape\n",
    "reg_vec=np.ones(m)\n",
    "reg_vec=np.append(reg_vec,[1]) #because add_intercept==true, users should add intercept in andvance\n",
    "\n",
    "lambda_vec=reg_vec\n",
    "\n",
    "logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic1.fit(X_train, Y_train)\n",
    "ypred = logic1.predict(X_test)\n",
    "accuracy=logic1.CalAccuracy(ypred,Y_test)\n",
    "print(\"ACCURACY:\",accuracy)\n",
    "print(\"WEIGHT:\\n\",logic1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.8477423638778221\n",
      "WEIGHT:\n",
      " [ 0.25833063  0.35307341  2.33348256  0.7378757   0.33385106  0.07926886\n",
      " -0.04219572  0.19987643 -0.58360967 -0.93671318  0.0754847   1.28715744\n",
      " -0.37140327  0.39422898  0.04305748 -0.26147348  0.19559029 -0.42695771\n",
      "  0.42695771  0.16424528  0.22840772  0.96472553 -0.81743779 -0.52074423\n",
      " -0.09910239 -0.64944042 -1.55235099  0.6786798   0.75066429  0.45541098\n",
      "  0.81857112  0.07308911  0.0728464  -0.11752645 -0.06282948  0.67242506\n",
      "  0.5040869   0.08799091  0.11435013 -0.38483984 -0.10196309 -0.05145374\n",
      "  0.10741777 -0.01997934  0.01717544 -1.16567808  0.30082277  0.02715464\n",
      "  1.00831207  0.50210397 -0.45756662 -1.24002555  0.52780939 -0.86832688\n",
      " -0.02771494 -0.31412701  0.47343435  0.62981111  0.62405658 -0.5867506\n",
      " -0.0296708   0.12414401 -0.14376238  0.02434194  0.0621604  -0.24843986\n",
      "  0.19459429  0.52620501  0.93165615  0.18707696  0.37950109 -0.28749402\n",
      " -0.31137357 -0.33290534 -0.65117786 -0.38160106  0.48879121  0.17662205\n",
      "  0.17410342 -0.07343502 -0.0314651  -0.89846776  0.00653561 -0.27232555\n",
      " -0.12442075  0.39697177 -0.75318727  0.61067658  0.70544004  0.01789988\n",
      "  0.2090388   0.382747   -0.2795817  -0.10453082 -0.9310132  -0.52642475\n",
      "  1.61398956  1.367359   -0.49235221 -1.0149365  -0.60567592 -0.34195918\n",
      " -3.17508579]\n"
     ]
    }
   ],
   "source": [
    "n,m=X_train.shape\n",
    "reg_vec=np.ones(m)\n",
    "reg_vec=np.append(reg_vec,[0]) #because add_intercept==true, users should add intercept in andvance\n",
    "\n",
    "lambda_vec=reg_vec\n",
    "\n",
    "logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic1.fit(X_train, Y_train)\n",
    "ypred = logic1.predict(X_test)\n",
    "accuracy=logic1.CalAccuracy(ypred,Y_test)\n",
    "print(\"ACCURACY:\",accuracy)\n",
    "print(\"WEIGHT:\\n\",logic1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.847675962815405\n",
      "WEIGHT:\n",
      " [ 0.25851661  0.3533387   2.33562764  0.7825921   0.33439916  0.07940036\n",
      " -0.08347988  0.23309138 -0.59278097 -0.92248498  0.11139578  1.25425868\n",
      " -0.38299462  0.41291781  0.04136013 -0.26411462  0.19283129 -0.42890321\n",
      "  0.42890321  0.23635124  0.30021363  1.03810523 -0.75216084 -0.45341369\n",
      " -0.02691155 -0.58252688 -2.00075405  0.75127893  0.82696619  0.52830707\n",
      "  0.89488996  0.14510376  0.18253094 -0.02583999  0.00991404  0.89862004\n",
      "  0.68517002  0.23294385  0.24519931 -0.38363083 -0.08029608 -0.06493444\n",
      "  0.0453608   0.03743376 -0.01295908 -2.09374319  0.25763304  0.06659781\n",
      "  1.18748312  0.55059265 -0.47576613 -1.45842154  0.5822242  -1.0627833\n",
      " -0.00957212 -0.31704572  0.52485137  0.73044517  0.67457228 -0.63624179\n",
      " -0.00967268  0.17339113 -0.2364757   0.0375474   0.10120874 -0.24679341\n",
      "  0.23800627  0.64228457  1.00567032  0.23258941  0.42267607 -0.35336167\n",
      " -0.29178766 -0.38125401 -0.96291964 -0.45007954  0.512985    0.22019382\n",
      "  0.22640627 -0.04989103 -0.01836864 -0.95953334  0.01656804 -0.32741555\n",
      " -0.14011404  0.42856024 -0.84476926  0.75121645  0.76670733  0.07638783\n",
      "  0.26824615  0.44314098 -0.22058151 -0.04631789 -1.28758289 -0.57187069\n",
      "  1.82502291  1.39622515 -0.54691698 -1.05894078 -0.6555147  -0.38800491\n",
      " -3.36269038]\n"
     ]
    }
   ],
   "source": [
    "n,m=X_train.shape\n",
    "reg_vec=np.ones(m)\n",
    "for i in range(6,m):\n",
    "    reg_vec[i]=0.5\n",
    "reg_vec=np.append(reg_vec,[0]) #because add_intercept==true, users should add intercept in andvance\n",
    "\n",
    "lambda_vec=reg_vec\n",
    "\n",
    "logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic1.fit(X_train, Y_train)\n",
    "ypred = logic1.predict(X_test)\n",
    "accuracy=logic1.CalAccuracy(ypred,Y_test)\n",
    "print(\"ACCURACY:\",accuracy)\n",
    "print(\"WEIGHT:\\n\",logic1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Further split the training data into subtraining (90%) and tuning (10%) to search for the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning(a1,a2,x_subtrain,x_tune,y_subtrain,y_tune):\n",
    "    n,m=x_subtrain.shape\n",
    "    reg_vec=np.ones(m)\n",
    "    \n",
    "    for i in range(0,6):\n",
    "        reg_vec[i]=a1\n",
    "    for i in range(6,m):\n",
    "        reg_vec[i]=a2\n",
    "    reg_vec=np.append(reg_vec,[0]) #because add_intercept==true, users should add intercept in andvance\n",
    "    #print(reg_vec)\n",
    "\n",
    "    lambda_vec=reg_vec\n",
    "    logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "    logic1.fit(x_subtrain,y_subtrain)\n",
    "    ypred = logic1.predict(x_tune)\n",
    "    accuracy=logic1.CalAccuracy(ypred,y_tune)\n",
    "    print(\"Con:\",a1,\",Bin:\",a2,\"-------ACCURACY:\",accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_subtraining,X_tuning,y_subtraining,y_tuning=train_test_split(X_train,Y_train,test_size=0.1,random_state=87)\n",
    "\n",
    "# tuning grids\n",
    "grids=[0.01,0.05,0.1,0.5,1,5,10,20,50,100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con: 0.01 ,Bin: 0.01 -------ACCURACY: 0.8485250248591316\n",
      "Con: 0.05 ,Bin: 0.05 -------ACCURACY: 0.8485250248591316\n",
      "Con: 0.1 ,Bin: 0.1 -------ACCURACY: 0.8485250248591316\n",
      "Con: 0.5 ,Bin: 0.5 -------ACCURACY: 0.8488564799469672\n",
      "Con: 1 ,Bin: 1 -------ACCURACY: 0.8488564799469672\n",
      "Con: 5 ,Bin: 5 -------ACCURACY: 0.8475306595956248\n",
      "Con: 10 ,Bin: 10 -------ACCURACY: 0.8475306595956248\n",
      "Con: 20 ,Bin: 20 -------ACCURACY: 0.8478621146834604\n",
      "Con: 50 ,Bin: 50 -------ACCURACY: 0.848193569771296\n",
      "Con: 100 ,Bin: 100 -------ACCURACY: 0.8471992045077892\n"
     ]
    }
   ],
   "source": [
    "# Step1: a1=a2\n",
    "equal_weight_acc=[]\n",
    "\n",
    "for i in range(len(grids)):\n",
    "    a1=grids[i]\n",
    "    a2=grids[i]\n",
    "    accuracy=tuning(a1,a2,X_subtraining,X_tuning,y_subtraining,y_tuning)\n",
    "    equal_weight_acc.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con: 0.5 ,Bin: 0.01 -------ACCURACY: 0.8485250248591316\n",
      "Con: 0.5 ,Bin: 0.05 -------ACCURACY: 0.8485250248591316\n",
      "Con: 0.5 ,Bin: 0.1 -------ACCURACY: 0.8488564799469672\n",
      "Con: 0.5 ,Bin: 0.5 -------ACCURACY: 0.8488564799469672\n",
      "Con: 0.5 ,Bin: 1 -------ACCURACY: 0.8488564799469672\n",
      "Con: 0.5 ,Bin: 5 -------ACCURACY: 0.8475306595956248\n",
      "Con: 0.5 ,Bin: 10 -------ACCURACY: 0.8468677494199536\n",
      "Con: 0.5 ,Bin: 20 -------ACCURACY: 0.8468677494199536\n",
      "Con: 0.5 ,Bin: 50 -------ACCURACY: 0.8478621146834604\n",
      "Con: 0.5 ,Bin: 100 -------ACCURACY: 0.8475306595956248\n"
     ]
    }
   ],
   "source": [
    "# Step2: according to step1, choose 0.5 as the best value and fix a1=0.5\n",
    "fix_a1_acc=[]\n",
    "\n",
    "for i in range(len(grids)):\n",
    "    a1=0.5\n",
    "    a2=grids[i]\n",
    "    accuracy=tuning(a1,a2,X_subtraining,X_tuning,y_subtraining,y_tuning)\n",
    "    fix_a1_acc.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con: 0.01 ,Bin: 0.5 -------ACCURACY: 0.8488564799469672\n",
      "Con: 0.05 ,Bin: 0.5 -------ACCURACY: 0.8488564799469672\n",
      "Con: 0.1 ,Bin: 0.5 -------ACCURACY: 0.8488564799469672\n",
      "Con: 0.5 ,Bin: 0.5 -------ACCURACY: 0.8488564799469672\n",
      "Con: 1 ,Bin: 0.5 -------ACCURACY: 0.8488564799469672\n",
      "Con: 5 ,Bin: 0.5 -------ACCURACY: 0.8495193901226383\n",
      "Con: 10 ,Bin: 0.5 -------ACCURACY: 0.8498508452104739\n",
      "Con: 20 ,Bin: 0.5 -------ACCURACY: 0.8501823002983095\n",
      "Con: 50 ,Bin: 0.5 -------ACCURACY: 0.8488564799469672\n",
      "Con: 100 ,Bin: 0.5 -------ACCURACY: 0.848193569771296\n"
     ]
    }
   ],
   "source": [
    "# Step2: choose 0.5 as the best value and fix a2=0.5\n",
    "fix_a1_acc=[]\n",
    "\n",
    "for i in range(len(grids)):\n",
    "    a1=grids[i]\n",
    "    a2=0.5\n",
    "    accuracy=tuning(a1,a2,X_subtraining,X_tuning,y_subtraining,y_tuning)\n",
    "    fix_a1_acc.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con: 20 ,Bin: 0.5 -------TEST ACCURACY: 0.8477423638778221\n"
     ]
    }
   ],
   "source": [
    "# the best combination is (a1,a2)=(20,0.5) and the accuracy is about 0.8502\n",
    "# therefore, select (a1,a2)=(20,0.5) as the parameters to train the final model and test the accuracy\n",
    "a1=20\n",
    "a2=0.5\n",
    "\n",
    "n,m=X_train.shape\n",
    "reg_vec=np.ones(m)\n",
    "    \n",
    "for i in range(0,6):\n",
    "    reg_vec[i]=a1\n",
    "for i in range(6,m):\n",
    "    reg_vec[i]=a2\n",
    "reg_vec=np.append(reg_vec,[0]) #because add_intercept==true, users should add intercept in andvance\n",
    "lambda_vec=reg_vec\n",
    "\n",
    "logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic1.fit(X_train, Y_train)\n",
    "ypred = logic1.predict(X_test)\n",
    "accuracy=logic1.CalAccuracy(ypred,Y_test)\n",
    "print(\"Con:\",a1,\",Bin:\",a2,\"-------TEST ACCURACY:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "說明：  \n",
    "以 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 50, 100 作為訓練的參數。  \n",
    "當 $a_1$ = $a_2$ 時，$a_1$ = $a_2$ = 0.5 和 $a_1$ = $a_2$ = 1 在準確率的表現上是最好，在這裡我選擇 0.5 做為 $a_1^*$ 和 $a_2^*$     \n",
    "固定 $a_1$ = $a_1^*$ = 0.5 訓練，發現 $a_2$ = 0.1, 0.5, 1 時在準確率的表現上是最好，在這裡我選擇 0.5 做為新的 $a_2^*$ 進入下一輪調校   \n",
    "固定 $a_2$ = $a_2^*$ = 0.5 訓練，發現 (a1,a2) = (20,0.5) 時的準確率表現(約0.8502)最好，故選擇此組合做為最後訓練模型的參數  \n",
    "使用 ($a_1$,$a_2$) = (20,0.5) 訓練模型，在testing資料的準確率表現為0.8477423638778221"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Use sklearn.linear_model.LogisticRegression to train and test the model (including hyperparameter tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.01 -------ACCURACY: 0.8471992045077892\n",
      "C: 0.05 -------ACCURACY: 0.8478621146834604\n",
      "C: 0.1 -------ACCURACY: 0.8475306595956248\n",
      "C: 0.5 -------ACCURACY: 0.848193569771296\n",
      "C: 1 -------ACCURACY: 0.8488564799469672\n",
      "C: 5 -------ACCURACY: 0.8488564799469672\n",
      "C: 10 -------ACCURACY: 0.8485250248591316\n",
      "C: 20 -------ACCURACY: 0.8485250248591316\n",
      "C: 50 -------ACCURACY: 0.8485250248591316\n",
      "C: 100 -------ACCURACY: 0.8485250248591316\n"
     ]
    }
   ],
   "source": [
    "skl_accuracy=[]\n",
    "grids=[0.01,0.05,0.1,0.5,1,5,10,20,50,100]\n",
    "\n",
    "for i in range(len(grids)):\n",
    "    lrg=LogisticRegression(solver='newton-cg',C=grids[i])\n",
    "    lrg.fit(X_subtraining,y_subtraining)\n",
    "    ypred=lrg.predict(X_tuning)\n",
    "    ncorrect=np.sum(y_tuning==ypred)\n",
    "    accuracy=ncorrect/y_tuning.shape[0]\n",
    "    print(\"C:\",grids[i],\"-------ACCURACY:\",accuracy)\n",
    "    skl_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:1-------TESTING ACCURACY: 0.8477423638778221\n"
     ]
    }
   ],
   "source": [
    "# according the result of tuning, choose c=1 as the final model parameter\n",
    "lrg=LogisticRegression(solver='newton-cg',C=1)\n",
    "lrg.fit(X_train,Y_train)\n",
    "ypred=lrg.predict(X_test)\n",
    "ncorrect=np.sum(Y_test==ypred)\n",
    "accuracy=ncorrect/Y_test.shape[0]\n",
    "print(\"C:1-------TESTING ACCURACY:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "說明：  \n",
    "在sklearn.linear_model.LogisticRegression的使用上，C這個參數會跟regularization有關，因此我使用跟3.3題一樣的grids去調校C這個參數。調校結果發現，當 C=1 和 C=5 的時候有最高的準確率。  \n",
    "若比較3.4與3.3，可以發現3.3題在固定 $a_1$=$a_2$ 時，除了 $a_1$=$a_2$=0.5 ， $a_1$=$a_2$=1 也會跟 $a_1$=$a_2$=0.5 一樣有最好的準確率，推測可能 C=1 會有比較好的結果，因此選擇 C=1 做為最後模型訓練的參數，最後在testing資料的準確率表現為0.8477423638778221，與3.3題的準確率相同。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
